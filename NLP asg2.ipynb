{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Langauge Processing - Assignment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize as st, word_tokenize as wt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) Parsing of dataset (Alice in Wonderland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = (open('holmes.txt','r')).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = 0\n",
    "types_set = set()\n",
    "list_all_words = []\n",
    "sent_token_list = []\n",
    "unigrams = {} # dictionary for maintaining count of unigrams\n",
    "MLE_Unigrams = {} # dictionary for maintaining MLE for Unigrams\n",
    "MLE_Bigrams = {} # dictionary for maintaining MLE for Bigrams\n",
    "MLE_Trigrams = {} # dictionary for maintaining MLE for Trigrams\n",
    "MLE_Quadgrams = {} # dictionary for maintaining MLE for Quadgrams\n",
    "Add_1_Bigrams = {} # dictionary for maintaining Add-one estimation of Bigrams\n",
    "eff_bigrams = {} # dictionary for maintaining Effective Bigram counts as effect of Add-one smoothing\n",
    "p = r'[?|$|&|*|%|@|(|)|~|.|:|,|;|\\'|,|`|!|\\--]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) Sentence Tokenization and filtering out unnecessary symbols, newlines, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7887"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize_list = st(text)\n",
    "total_sents = len(sent_tokenize_list)\n",
    "pattern = r'[^a-zA-Z0-9]'\n",
    "for i in range(total_sents):\n",
    "    sentence = sent_tokenize_list[i].strip()\n",
    "    sent_tokenize_list[i] = re.sub(pattern, r' ', sentence)\n",
    "    temp = sent_tokenize_list[i].split()\n",
    "    sent_tokenize_list[i] = \" \".join(temp) # remove whitespaces\n",
    "    sent_token_list.append(sent_tokenize_list[i])\n",
    "    sent_tokenize_list[i] = \"<s> \"+ \" \".join(temp) + \" </s>\"\n",
    "sent_tokenize_list = [x.lower() for x in sent_tokenize_list]\n",
    "sent_token_list = [x.lower() for x in sent_token_list]\n",
    "#sent_token_list\n",
    "\n",
    "### For whole Corpus:\n",
    "t = 0\n",
    "types = set()\n",
    "unigram = {}\n",
    "for i in range(total_sents):\n",
    "    t_in_line = wt(sent_token_list[i])\n",
    "    t = t + len(t_in_line)\n",
    "    types.update(t_in_line)      \n",
    "for word in types:\n",
    "    unigram[word] = 0\n",
    "for i in range(total_sents):\n",
    "    token_in_line = wt(sent_token_list[i]);\n",
    "    for word in token_in_line:\n",
    "        unigram[word] += 1\n",
    "unigram['<s>'] = 6820\n",
    "unigram['<\\s>'] = 6820\n",
    "len(unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of Training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_size = int(total_sents*0.8)\n",
    "train_data = random.sample(sent_token_list, train_data_size)\n",
    "test_data = [n for n in sent_token_list if n not in train_data]\n",
    "test_data_size = len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) MLE for unigrams (on training corpus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7235"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(train_data_size):\n",
    "    token_in_line = wt(train_data[i])\n",
    "    tokens = tokens + len(token_in_line)\n",
    "    types_set.update(token_in_line)      #updating the list of types line by line i.e. unique words\n",
    "V = len(types_set)\n",
    "for word in types_set:\n",
    "    unigrams[word] = 0\n",
    "for i in range(train_data_size):\n",
    "    token_in_line = wt(train_data[i]);\n",
    "    for word in token_in_line:\n",
    "        unigrams[word] += 1\n",
    "unigrams['<s>'] = 6820\n",
    "unigrams['<\\s>'] = 6820\n",
    "\n",
    "for word in unigrams.keys():\n",
    "    freq = unigrams[word]\n",
    "    MLE_Unigrams[word] = (freq/tokens)*100   # multiplied with 100 just to show the probabalities in more readable format\n",
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for Bigrams(on whole Corpus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Counter()\n",
    "MLE_Bigram = {}\n",
    "for sentnc in sent_token_list:\n",
    "    token = wt(sentnc)\n",
    "    b = Counter(ngrams(token, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    dict.update(bigram, b)\n",
    "for k,v in bigram.items():\n",
    "    key = k[0]\n",
    "    if key in unigram.keys():\n",
    "        MLE_Bigram[k] = v/unigram[key]\n",
    "    else:\n",
    "        MLE_Bigram[k] = 0\n",
    "#print(MLE_Bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for Bigrams(on Training corpus.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42219"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = Counter()\n",
    "for sentnc in train_data:\n",
    "    token = wt(sentnc)\n",
    "    b = Counter(ngrams(token, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    dict.update(bigrams, b)\n",
    "for k,v in bigrams.items():\n",
    "    key = k[0]\n",
    "    if key in unigrams.keys():\n",
    "        MLE_Bigrams[k] = v/unigrams[key]\n",
    "    else:\n",
    "        MLE_Bigrams[k] = 0\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for Trigrams (on Training corpus.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = Counter()\n",
    "for sentnc in train_data:\n",
    "    token = wt(sentnc)\n",
    "    t = Counter(ngrams(token, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    dict.update(trigrams, t)\n",
    "#print(trigrams)\n",
    "for k,v in trigrams.items():\n",
    "    key = (k[0],k[1])\n",
    "    if key in dict(bigrams).keys():\n",
    "        MLE_Trigrams[k] = v/bigrams[key]\n",
    "    else:\n",
    "        MLE_Trigrams[k] = 0\n",
    "#MLE_Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for Quadgrams(on Training corpus.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quadgrams = Counter()\n",
    "for sentnc in train_data:\n",
    "    token = wt(sentnc)\n",
    "    q = Counter(ngrams(token, 4, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    dict.update(quadgrams, q)\n",
    "#print(quadgrams)\n",
    "for k,v in quadgrams.items():\n",
    "    key = (k[0],k[1],k[2])\n",
    "    if key in dict(trigrams).keys():\n",
    "        MLE_Quadgrams[k] = v/trigrams[key]\n",
    "    else:\n",
    "        MLE_Quadgrams[k] = 0\n",
    "#MLE_Quadgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many n-grams are possible and how many actually exists? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7887  unigrams are possible and  7887  actually exists\n",
      "49220  bigrams are possible and  62204769  actually exists\n"
     ]
    }
   ],
   "source": [
    "V = len(unigram)\n",
    "print(V,\" unigrams are possible and \",V,\" actually exists\")\n",
    "print(len(bigram),\" bigrams are possible and \",V*V,\" actually exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q4 a) Developing a Generator: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'swiftly', 'mrs', 'far', 'though', 'remarkable', 'dear', 'think', 'in']\n"
     ]
    }
   ],
   "source": [
    "def gen(model):\n",
    "    sentence = []\n",
    "    p_list = []\n",
    "    if model=='bigram':\n",
    "        word = '<s>'\n",
    "        sentence.append(word)\n",
    "        t= '</s>'\n",
    "        for i in range(8):\n",
    "            for k,v in MLE_Bigrams.items():\n",
    "                key = k[0]\n",
    "                if key==word:\n",
    "                    p_list.append(k[1])\n",
    "            new_word = random.choice(p_list)\n",
    "            sentence.append(new_word)\n",
    "            t = new_word   #### until it founds last word </s>\n",
    "        print(sentence)\n",
    "        \n",
    "gen('bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 b) Computing the probability of a given sentence in log-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.7762626572401894\n"
     ]
    }
   ],
   "source": [
    "def prob(sentence,model_name):\n",
    "    log_p = 1\n",
    "    sentence = sentence.lower();\n",
    "    token = wt(sentence)\n",
    "    b = list(ngrams(token, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    for key in b:\n",
    "        log_p = MLE_Bigram[key]\n",
    "        log_p += math.log10(log_p)\n",
    "    print(log_p)\n",
    "    \n",
    "prob(\"I have seldom heard him mention her under any other name\",MLE_Bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5) Add-1 Smoothing for Bigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram count of ('gibe','and'): 1   and  Effective Bigram count,c* of ('gibe','and'): 0.0002535496957403651\n",
      "Bigram count of ('senility','</s>'): 1   and  Effective Bigram count,c* of ('senility','</s>'): 0.0002535496957403651\n"
     ]
    }
   ],
   "source": [
    "for k,v in bigrams.items():\n",
    "    key = k[0]\n",
    "    if key in unigrams.keys():\n",
    "        Add_1_Bigrams[k] = (v+1)/(unigrams[key]+V)\n",
    "        eff_bigrams[k] = Add_1_Bigrams[k]*unigrams[key]\n",
    "    else:\n",
    "        Add_1_Bigrams[k] = 0\n",
    "        eff_bigrams[k] = 0\n",
    "#print(Add_1_Bigrams)\n",
    "#print(eff_bigrams)\n",
    "print(\"Bigram count of ('gibe','and'):\",bigrams['gibe','and'],\"  and  Effective Bigram count,c* of ('gibe','and'):\",eff_bigrams['gibe','and'])\n",
    "print(\"Bigram count of ('senility','</s>'):\",bigrams['senility','</s>'],\"  and  Effective Bigram count,c* of ('senility','</s>'):\",eff_bigrams['senility','</s>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Drastic change occurs in the counts(post-smoothing) because in Add-one smoothing too much weight is given to unseen ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6) Good-turing smoothing technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:  0.3832345362692761\n"
     ]
    }
   ],
   "source": [
    "c = [0,1,2,3,4,5,6,7,8,9]\n",
    "good_c = []\n",
    "N = {1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0}\n",
    "#uni_count = 0\n",
    "for word,freq in unigrams.items():\n",
    "    if freq >= 1 and freq <= 10:\n",
    "        N[freq] += 1\n",
    "        #uni_count += 1\n",
    "for i in c:\n",
    "    if i == 0:\n",
    "        z = N[1]/len(unigrams)\n",
    "    else:\n",
    "        z = ((i+1)*N[i+1])/N[i]\n",
    "    good_c.append(z)\n",
    "d = []\n",
    "for i in range(len(c)):\n",
    "    d.append(c[i]-good_c[i])\n",
    "print(\"d: \",sum(d) / len(d) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7) Computation of the perplexity value for the test dataset for the bigram model using add-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Test data:  1816.5767843481103\n"
     ]
    }
   ],
   "source": [
    "PP1 = 1 # perplexity\n",
    "Add_1_test_Bigrams = {}\n",
    "x = 0\n",
    "types_test = set()\n",
    "test_unigrams = {}\n",
    "\n",
    "for i in range(test_data_size):\n",
    "    t_in_line = wt(test_data[i])\n",
    "    x = x + len(t_in_line)\n",
    "    types_test.update(t_in_line)  \n",
    "V_test = len(types_test)\n",
    "for word in types_test:\n",
    "    test_unigrams[word] = 0\n",
    "for i in range(test_data_size):\n",
    "    token_in_line = wt(test_data[i]);\n",
    "    for word in token_in_line:\n",
    "        test_unigrams[word] += 1\n",
    "test_unigrams['<s>'] = 6820\n",
    "test_unigrams['<\\s>'] = 6820\n",
    "\n",
    "test_bigrams = Counter()\n",
    "for sentnc in test_data:\n",
    "    token = wt(sentnc)\n",
    "    b = Counter(ngrams(token, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'))\n",
    "    dict.update(test_bigrams, b)\n",
    "n = len(test_bigrams)\n",
    "\n",
    "for k,v in test_bigrams.items():\n",
    "    key = k[0]\n",
    "    if key in test_unigrams.keys():\n",
    "        Add_1_test_Bigrams[k] = (v+1)/(test_unigrams[key]+V_test)\n",
    "    else:\n",
    "        Add_1_test_Bigrams[k] = 0\n",
    "\n",
    "for k,v in Add_1_test_Bigrams.items():\n",
    "    p = (1/v)**(1/n)\n",
    "    PP1 *= p\n",
    "print(\"Perplexity on Test data: \",PP1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the perplexity value for the test dataset for the bigram model using Good-turing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on Test data:  0.9626914512342537\n"
     ]
    }
   ],
   "source": [
    "PP2 = 1 # perplexity\n",
    "good_turing_test_Bigrams = []\n",
    "n_test = 0\n",
    "c_test = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "good_c_test = []\n",
    "N_test = {1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0,13:0,14:0,15:0,16:0,17:0,18:0,19:0,20:0}\n",
    "for word,freq in test_unigrams.items():\n",
    "    if freq >= 1 and freq <= 20:\n",
    "        N_test[freq] += 1\n",
    "        n_test += 1\n",
    "for i in c_test:\n",
    "    if i == 0:\n",
    "        z = N_test[1]/n_test\n",
    "    else:\n",
    "        z = ((i+1)*N_test[i+1])/N_test[i]\n",
    "    good_c_test.append(z)\n",
    "        \n",
    "for i in good_c_test:\n",
    "    good_turing_test_Bigrams.append(i/n_test)\n",
    "    \n",
    "for i in good_turing_test_Bigrams:\n",
    "    p = (1/i)**(-1/n_test)\n",
    "    PP2 *= p\n",
    "print(\"Perplexity on Test data: \",PP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Turing performs better as it has low perplexity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
